{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bbb3052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping articles from 'opinion' section...\n",
      "\n",
      "\n",
      "Scraping articles from 'health' section...\n",
      "\n",
      "\n",
      "Scraping articles from 'health' section...\n",
      "\n",
      "\n",
      "Scraping articles from 'health' section...\n",
      "\n",
      "\n",
      "Scraping articles from 'health' section...\n",
      "\n",
      "\n",
      "Scraping articles from 'health' section...\n",
      "\n",
      "\n",
      "Scraping articles from 'health' section...\n",
      "\n",
      "\n",
      "Scraping articles from 'world' section...\n",
      "\n",
      "\n",
      "Scraping articles from 'explainers' section...\n",
      "\n",
      "\n",
      "Scraping articles from 'explainers' section...\n",
      "\n",
      "\n",
      "Scraping articles from 'health' section...\n",
      "\n",
      "\n",
      "Scraping articles from 'health' section...\n",
      "\n",
      "\n",
      "Scraping articles from 'explainers' section...\n",
      "\n",
      "\n",
      "Scraping articles from 'explainers' section...\n",
      "\n",
      "\n",
      "Scraping articles from 'explainers' section...\n",
      "\n",
      "\n",
      "Scraping articles from 'health' section...\n",
      "\n",
      "\n",
      "Scraping articles from 'explainers' section...\n",
      "\n",
      "\n",
      "Scraping articles from 'explainers' section...\n",
      "\n",
      "\n",
      "Scraping articles from 'explainers' section...\n",
      "\n",
      "\n",
      "Scraping articles from 'explainers' section...\n",
      "\n",
      "\n",
      "Scraping articles from 'explainers' section...\n",
      "\n",
      "\n",
      "Scraping articles from 'world' section...\n",
      "\n",
      "\n",
      "Scraping articles from 'health' section...\n",
      "\n",
      "\n",
      "Data saved to 'firstpost-text-health.csv'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "from urllib.parse import urljoin  # For joining URLs\n",
    "\n",
    "def scrape_news():\n",
    "    base_url = 'https://www.firstpost.com/category/health'\n",
    "    url = 'https://www.firstpost.com/category/health'  # URL of the News website\n",
    "    data = []\n",
    "\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find sections/categories on the News website\n",
    "        sections = soup.find_all('a', class_='category-name')\n",
    "\n",
    "        for section in sections:\n",
    "            section_name = section.text.strip()\n",
    "            section_link = urljoin(base_url, section['href'])  # Join base URL with section link\n",
    "\n",
    "            print(f\"Scraping articles from '{section_name}' section...\")\n",
    "            section_data = scrape_section(section_name,section_link)\n",
    "            data.extend(section_data)\n",
    "            print(\"\\n\")\n",
    "\n",
    "        # Save data to CSV file\n",
    "        save_to_csv(data)\n",
    "\n",
    "    else:\n",
    "        print('Failed to fetch data from News')\n",
    "\n",
    "def scrape_section(section,section_url):\n",
    "    section_data = []\n",
    "    # Send a GET request to the section URL\n",
    "    response = requests.get(section_url)\n",
    "\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find all article elements\n",
    "        articles = soup.find_all('div', class_='title-wrap')\n",
    "\n",
    "        # Extract titles and content of each article\n",
    "        for article in articles:\n",
    "            # Extract title\n",
    "            title_element = article.find('h3', class_='main-title')\n",
    "            title = title_element.text.strip() if title_element else 'No title'\n",
    "\n",
    "            # Extract content (if available)\n",
    "            content_element = article.find('p', class_='copy')\n",
    "            content = content_element.text.strip() if content_element else 'No content'\n",
    "\n",
    "            section_data.append({'Section': section,'Title': title, 'Content': content})\n",
    "\n",
    "    else:\n",
    "        print(f'Failed to fetch data from {section_url}')\n",
    "\n",
    "    return section_data\n",
    "\n",
    "def save_to_csv(data):\n",
    "    # Define CSV file name\n",
    "    file_name = 'data/firstpost-text-health.csv'\n",
    "\n",
    "    # Write data to CSV file\n",
    "    with open(file_name, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        fieldnames = ['Section','Title', 'Content']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "        # Write header\n",
    "        writer.writeheader()\n",
    "\n",
    "        # Write rows\n",
    "        for row in data:\n",
    "            writer.writerow(row)\n",
    "\n",
    "    print(f\"Data saved to '{file_name}'\")\n",
    "\n",
    "# Call the function to initiate scraping and save data to CSV\n",
    "scrape_news()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98b149fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping articles from 'sports' section...\n",
      "\n",
      "\n",
      "Scraping articles from 'sports' section...\n",
      "\n",
      "\n",
      "Scraping articles from 'sports' section...\n",
      "\n",
      "\n",
      "Scraping articles from 'sports' section...\n",
      "\n",
      "\n",
      "Scraping articles from 'sports' section...\n",
      "\n",
      "\n",
      "Scraping articles from 'sports' section...\n",
      "\n",
      "\n",
      "Scraping articles from 'sports' section...\n",
      "\n",
      "\n",
      "Scraping articles from 'sports' section...\n",
      "\n",
      "\n",
      "Scraping articles from 'india' section...\n",
      "\n",
      "\n",
      "Scraping articles from 'sports' section...\n",
      "\n",
      "\n",
      "Scraping articles from 'sports' section...\n",
      "\n",
      "\n",
      "Scraping articles from 'sports' section...\n",
      "\n",
      "\n",
      "Scraping articles from 'sports' section...\n",
      "\n",
      "\n",
      "Scraping articles from 'sports' section...\n",
      "\n",
      "\n",
      "Scraping articles from 'sports' section...\n",
      "\n",
      "\n",
      "Scraping articles from 'sports' section...\n",
      "\n",
      "\n",
      "Scraping articles from 'sports' section...\n",
      "\n",
      "\n",
      "Scraping articles from 'sports' section...\n",
      "\n",
      "\n",
      "Scraping articles from 'world' section...\n",
      "\n",
      "\n",
      "Scraping articles from 'firstcricket' section...\n",
      "\n",
      "\n",
      "Scraping articles from 'sports' section...\n",
      "\n",
      "\n",
      "Scraping articles from 'sports' section...\n",
      "\n",
      "\n",
      "Scraping articles from 'sports' section...\n",
      "\n",
      "\n",
      "Data saved to 'data/firstpost-text-sports.csv'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "from urllib.parse import urljoin  # For joining URLs\n",
    "\n",
    "def scrape_news():\n",
    "    base_url = 'https://www.firstpost.com/category/sports'\n",
    "    url = 'https://www.firstpost.com/category/sports'  # URL of the News website\n",
    "    data = []\n",
    "\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find sections/categories on the News website\n",
    "        sections = soup.find_all('a', class_='category-name')\n",
    "\n",
    "        for section in sections:\n",
    "            section_name = section.text.strip()\n",
    "            section_link = urljoin(base_url, section['href'])  # Join base URL with section link\n",
    "\n",
    "            print(f\"Scraping articles from '{section_name}' section...\")\n",
    "            section_data = scrape_section(section_name,section_link)\n",
    "            data.extend(section_data)\n",
    "            print(\"\\n\")\n",
    "\n",
    "        # Save data to CSV file\n",
    "        save_to_csv(data)\n",
    "\n",
    "    else:\n",
    "        print('Failed to fetch data from News')\n",
    "\n",
    "def scrape_section(section,section_url):\n",
    "    section_data = []\n",
    "    # Send a GET request to the section URL\n",
    "    response = requests.get(section_url)\n",
    "\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find all article elements\n",
    "        articles = soup.find_all('div', class_='title-wrap')\n",
    "\n",
    "        # Extract titles and content of each article\n",
    "        for article in articles:\n",
    "            # Extract title\n",
    "            title_element = article.find('h3', class_='main-title')\n",
    "            title = title_element.text.strip() if title_element else 'No title'\n",
    "\n",
    "            # Extract content (if available)\n",
    "            content_element = article.find('p', class_='copy')\n",
    "            content = content_element.text.strip() if content_element else 'No content'\n",
    "\n",
    "            section_data.append({'Section': section,'Title': title, 'Content': content})\n",
    "\n",
    "    else:\n",
    "        print(f'Failed to fetch data from {section_url}')\n",
    "\n",
    "    return section_data\n",
    "\n",
    "def save_to_csv(data):\n",
    "    # Define CSV file name\n",
    "    file_name = 'data/firstpost-text-sports.csv'\n",
    "\n",
    "    # Write data to CSV file\n",
    "    with open(file_name, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        fieldnames = ['Section','Title', 'Content']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "        # Write header\n",
    "        writer.writeheader()\n",
    "\n",
    "        # Write rows\n",
    "        for row in data:\n",
    "            writer.writerow(row)\n",
    "\n",
    "    print(f\"Data saved to '{file_name}'\")\n",
    "\n",
    "# Call the function to initiate scraping and save data to CSV\n",
    "scrape_news()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5b2cc37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping articles from 'entertainment' section...\n",
      "\n",
      "\n",
      "Scraping articles from 'entertainment' section...\n",
      "\n",
      "\n",
      "Scraping articles from 'entertainment' section...\n",
      "\n",
      "\n",
      "Scraping articles from 'entertainment' section...\n",
      "\n",
      "\n",
      "Scraping articles from 'entertainment' section...\n",
      "\n",
      "\n",
      "Scraping articles from 'entertainment' section...\n",
      "\n",
      "\n",
      "Scraping articles from 'entertainment' section...\n",
      "\n",
      "\n",
      "Scraping articles from 'entertainment' section...\n",
      "\n",
      "\n",
      "Scraping articles from 'entertainment' section...\n",
      "\n",
      "\n",
      "Scraping articles from 'entertainment' section...\n",
      "\n",
      "\n",
      "Scraping articles from 'entertainment' section...\n",
      "\n",
      "\n",
      "Scraping articles from 'entertainment' section...\n",
      "\n",
      "\n",
      "Scraping articles from 'entertainment' section...\n",
      "\n",
      "\n",
      "Scraping articles from 'entertainment' section...\n",
      "\n",
      "\n",
      "Scraping articles from 'entertainment' section...\n",
      "\n",
      "\n",
      "Scraping articles from 'entertainment' section...\n",
      "\n",
      "\n",
      "Scraping articles from 'entertainment' section...\n",
      "\n",
      "\n",
      "Scraping articles from 'entertainment' section...\n",
      "\n",
      "\n",
      "Scraping articles from 'entertainment' section...\n",
      "\n",
      "\n",
      "Scraping articles from 'entertainment' section...\n",
      "\n",
      "\n",
      "Scraping articles from 'entertainment' section...\n",
      "\n",
      "\n",
      "Scraping articles from 'entertainment' section...\n",
      "\n",
      "\n",
      "Scraping articles from 'entertainment' section...\n",
      "\n",
      "\n",
      "Data saved to 'data/firstpost-text-ent.csv'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "from urllib.parse import urljoin  # For joining URLs\n",
    "\n",
    "def scrape_news():\n",
    "    base_url = 'https://www.firstpost.com/category/entertainment'\n",
    "    url = 'https://www.firstpost.com/category/entertainment'  # URL of the News website\n",
    "    data = []\n",
    "\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find sections/categories on the News website\n",
    "        sections = soup.find_all('a', class_='category-name')\n",
    "\n",
    "        for section in sections:\n",
    "            section_name = section.text.strip()\n",
    "            section_link = urljoin(base_url, section['href'])  # Join base URL with section link\n",
    "\n",
    "            print(f\"Scraping articles from '{section_name}' section...\")\n",
    "            section_data = scrape_section(section_name,section_link)\n",
    "            data.extend(section_data)\n",
    "            print(\"\\n\")\n",
    "\n",
    "        # Save data to CSV file\n",
    "        save_to_csv(data)\n",
    "\n",
    "    else:\n",
    "        print('Failed to fetch data from News')\n",
    "\n",
    "def scrape_section(section,section_url):\n",
    "    section_data = []\n",
    "    # Send a GET request to the section URL\n",
    "    response = requests.get(section_url)\n",
    "\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find all article elements\n",
    "        articles = soup.find_all('div', class_='title-wrap')\n",
    "\n",
    "        # Extract titles and content of each article\n",
    "        for article in articles:\n",
    "            # Extract title\n",
    "            title_element = article.find('h3', class_='main-title')\n",
    "            title = title_element.text.strip() if title_element else 'No title'\n",
    "\n",
    "            # Extract content (if available)\n",
    "            content_element = article.find('p', class_='copy')\n",
    "            content = content_element.text.strip() if content_element else 'No content'\n",
    "\n",
    "            section_data.append({'Section': section,'Title': title, 'Content': content})\n",
    "\n",
    "    else:\n",
    "        print(f'Failed to fetch data from {section_url}')\n",
    "\n",
    "    return section_data\n",
    "\n",
    "def save_to_csv(data):\n",
    "    # Define CSV file name\n",
    "    file_name = 'data/firstpost-text-ent.csv'\n",
    "\n",
    "    # Write data to CSV file\n",
    "    with open(file_name, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        fieldnames = ['Section','Title', 'Content']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "        # Write header\n",
    "        writer.writeheader()\n",
    "\n",
    "        # Write rows\n",
    "        for row in data:\n",
    "            writer.writerow(row)\n",
    "\n",
    "    print(f\"Data saved to '{file_name}'\")\n",
    "\n",
    "# Call the function to initiate scraping and save data to CSV\n",
    "scrape_news()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45b0a671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping articles from 'business' section...\n",
      "\n",
      "\n",
      "Scraping articles from 'business' section...\n",
      "\n",
      "\n",
      "Scraping articles from 'business' section...\n",
      "\n",
      "\n",
      "Scraping articles from 'business' section...\n",
      "\n",
      "\n",
      "Scraping articles from 'business' section...\n",
      "\n",
      "\n",
      "Scraping articles from 'business' section...\n",
      "\n",
      "\n",
      "Scraping articles from 'business' section...\n",
      "\n",
      "\n",
      "Scraping articles from 'business' section...\n",
      "\n",
      "\n",
      "Scraping articles from 'business' section...\n",
      "\n",
      "\n",
      "Scraping articles from 'business' section...\n",
      "\n",
      "\n",
      "Scraping articles from 'business' section...\n",
      "\n",
      "\n",
      "Scraping articles from 'business' section...\n",
      "\n",
      "\n",
      "Scraping articles from 'business' section...\n",
      "\n",
      "\n",
      "Scraping articles from 'india' section...\n",
      "\n",
      "\n",
      "Scraping articles from 'business' section...\n",
      "\n",
      "\n",
      "Scraping articles from 'business' section...\n",
      "\n",
      "\n",
      "Scraping articles from 'business' section...\n",
      "\n",
      "\n",
      "Scraping articles from 'business' section...\n",
      "\n",
      "\n",
      "Scraping articles from 'business' section...\n",
      "\n",
      "\n",
      "Scraping articles from 'business' section...\n",
      "\n",
      "\n",
      "Scraping articles from 'business' section...\n",
      "\n",
      "\n",
      "Scraping articles from 'business' section...\n",
      "\n",
      "\n",
      "Scraping articles from 'business' section...\n",
      "\n",
      "\n",
      "Data saved to 'firstpost-text-business.csv'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "from urllib.parse import urljoin  # For joining URLs\n",
    "\n",
    "def scrape_news():\n",
    "    base_url = 'https://www.firstpost.com/category/business'\n",
    "    url = 'https://www.firstpost.com/category/business'  # URL of the News website\n",
    "    data = []\n",
    "\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find sections/categories on the News website\n",
    "        sections = soup.find_all('a', class_='category-name')\n",
    "\n",
    "        for section in sections:\n",
    "            section_name = section.text.strip()\n",
    "            section_link = urljoin(base_url, section['href'])  # Join base URL with section link\n",
    "\n",
    "            print(f\"Scraping articles from '{section_name}' section...\")\n",
    "            section_data = scrape_section(section_name,section_link)\n",
    "            data.extend(section_data)\n",
    "            print(\"\\n\")\n",
    "\n",
    "        # Save data to CSV file\n",
    "        save_to_csv(data)\n",
    "\n",
    "    else:\n",
    "        print('Failed to fetch data from News')\n",
    "\n",
    "def scrape_section(section,section_url):\n",
    "    section_data = []\n",
    "    # Send a GET request to the section URL\n",
    "    response = requests.get(section_url)\n",
    "\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find all article elements\n",
    "        articles = soup.find_all('div', class_='title-wrap')\n",
    "\n",
    "        # Extract titles and content of each article\n",
    "        for article in articles:\n",
    "            # Extract title\n",
    "            title_element = article.find('h3', class_='main-title')\n",
    "            title = title_element.text.strip() if title_element else 'No title'\n",
    "\n",
    "            # Extract content (if available)\n",
    "            content_element = article.find('p', class_='copy')\n",
    "            content = content_element.text.strip() if content_element else 'No content'\n",
    "\n",
    "            section_data.append({'Section': section,'Title': title, 'Content': content})\n",
    "\n",
    "    else:\n",
    "        print(f'Failed to fetch data from {section_url}')\n",
    "\n",
    "    return section_data\n",
    "\n",
    "def save_to_csv(data):\n",
    "    # Define CSV file name\n",
    "    file_name = 'data/firstpost-text-business.csv'\n",
    "\n",
    "    # Write data to CSV file\n",
    "    with open(file_name, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        fieldnames = ['Section','Title', 'Content']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "        # Write header\n",
    "        writer.writeheader()\n",
    "\n",
    "        # Write rows\n",
    "        for row in data:\n",
    "            writer.writerow(row)\n",
    "\n",
    "    print(f\"Data saved to '{file_name}'\")\n",
    "\n",
    "# Call the function to initiate scraping and save data to CSV\n",
    "scrape_news()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "529ab1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1=pd.read_csv('data/firstpost-text-business.csv')\n",
    "df2=pd.read_csv('data/firstpost-text-ent.csv')\n",
    "df3=pd.read_csv('data/firstpost-text-health.csv')\n",
    "df4=pd.read_csv('data/firstpost-text-sports.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8456ee8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Section</th>\n",
       "      <th>Title</th>\n",
       "      <th>Content</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sports</td>\n",
       "      <td>Ronaldo left stunned as WWE superstar The Unde...</td>\n",
       "      <td>Not only the fans but Ronaldo was exhilarated ...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sports</td>\n",
       "      <td>Explained: Saudi Arabia's clever plan to dodge...</td>\n",
       "      <td>Saudi Arabia's exhibition tennis tournament wi...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sports</td>\n",
       "      <td>UEFA Nations League: France, Italy, Belgium, I...</td>\n",
       "      <td>UEFA Nations League champions Spain were drawn...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sports</td>\n",
       "      <td>India forced to share SAFF Women’s U19 Champio...</td>\n",
       "      <td>As per AIFF Secretary General M Satyanarayan, ...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sports</td>\n",
       "      <td>Lionel Messi accused of 'embarrassing Beijing'...</td>\n",
       "      <td>No content</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Section                                              Title  \\\n",
       "0  sports  Ronaldo left stunned as WWE superstar The Unde...   \n",
       "1  sports  Explained: Saudi Arabia's clever plan to dodge...   \n",
       "2  sports  UEFA Nations League: France, Italy, Belgium, I...   \n",
       "3  sports  India forced to share SAFF Women’s U19 Champio...   \n",
       "4  sports  Lionel Messi accused of 'embarrassing Beijing'...   \n",
       "\n",
       "                                             Content category  \n",
       "0  Not only the fans but Ronaldo was exhilarated ...   sports  \n",
       "1  Saudi Arabia's exhibition tennis tournament wi...   sports  \n",
       "2  UEFA Nations League champions Spain were drawn...   sports  \n",
       "3  As per AIFF Secretary General M Satyanarayan, ...   sports  \n",
       "4                                         No content   sports  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()\n",
    "df2.head()\n",
    "df3.head()\n",
    "df4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a12acbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['category']='business'\n",
    "df2['category']='entertainment'\n",
    "df3['category']='health'\n",
    "df4['category']='sports'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5e074fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.concat([df1,df2,df3,df4],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50ca330e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Section</th>\n",
       "      <th>Title</th>\n",
       "      <th>Content</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>Legendary singer AL Raghavan, husband of MN Ra...</td>\n",
       "      <td>No content</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>781</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>Academy of Motion Picture Arts and Sciences an...</td>\n",
       "      <td>This is the first time the Academy has introdu...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>With Bulbbul, Pataal Lok, the idea is to tell ...</td>\n",
       "      <td>No content</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1274</th>\n",
       "      <td>health</td>\n",
       "      <td>Budget 2024: WHO applauds extension of healthc...</td>\n",
       "      <td>The WHO said it will continue to support the G...</td>\n",
       "      <td>health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1401</th>\n",
       "      <td>sports</td>\n",
       "      <td>Paris Olympics will be challenging, will need ...</td>\n",
       "      <td>No content</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Section                                              Title  \\\n",
       "973   entertainment  Legendary singer AL Raghavan, husband of MN Ra...   \n",
       "781   entertainment  Academy of Motion Picture Arts and Sciences an...   \n",
       "491   entertainment  With Bulbbul, Pataal Lok, the idea is to tell ...   \n",
       "1274         health  Budget 2024: WHO applauds extension of healthc...   \n",
       "1401         sports  Paris Olympics will be challenging, will need ...   \n",
       "\n",
       "                                                Content       category  \n",
       "973                                          No content  entertainment  \n",
       "781   This is the first time the Academy has introdu...  entertainment  \n",
       "491                                          No content  entertainment  \n",
       "1274  The WHO said it will continue to support the G...         health  \n",
       "1401                                         No content         sports  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0e0d3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/news_data.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
